{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "minimal nmt.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/laziszaire/notebooks/blob/dev/minimal_nmt.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "YynyowxESngW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0543e75e-526f-4300-b00d-8a615ed61173"
      },
      "cell_type": "code",
      "source": [
        "!echo -e \"这是只猫\\n那是条狗\" > source\n",
        "!cat source\n",
        "!printf \"This is a cat\\nThat's a dog\" > target\n",
        "!cat target"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "这是只猫\n",
            "那是条狗\n",
            "This is a cat\n",
            "That's a dog"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3i0amaD2TUzY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8OqyfUJFU4mE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Iterator:  \n",
        "数据输入管道Data Input Pipeline"
      ]
    },
    {
      "metadata": {
        "id": "GC3ctE5OU4CY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ETL: Extract\n",
        "ds_source = tf.data.TextLineDataset(\"source\")\n",
        "ds_target = tf.data.TextLineDataset(\"target\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q-EnRPbzVO5p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5adff74-5324-441e-f980-708ccfc5ad1b"
      },
      "cell_type": "code",
      "source": [
        "#ETL: Load\n",
        "target_iter = ds_target.make_one_shot_iterator()\n",
        "with tf.Session() as sess:\n",
        "  print(target_iter.get_next().eval().decode('utf-8'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a cat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G1tKxT62wLyD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "DVvzsvHpWq4U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# dataset = ds_target.map(lambda string: tf.string_split([string]).values)\n",
        "# dataset = dataset.map(lambda words: (words, tf.size(words)))\n",
        "# _iter = dataset.make_one_shot_iterator()\n",
        "# words, length = _iter.get_next()\n",
        "# vocab = []\n",
        "# # build vocabulary: tensorflow\n",
        "# with tf.Session() as sess:\n",
        "#   while True:\n",
        "#     try:\n",
        "#       _words, _length = sess.run([words, length])\n",
        "#       vocab.append(_words)\n",
        "#     except tf.errors.OutOfRangeError:\n",
        "#       break\n",
        "#   vocab = tf.unique(tf.concat(vocab, 0))\n",
        "#   print(vocab.eval())\n",
        "#   table = tf.contrib.lookup.index_table_from_tensor(vocab)\n",
        "# #因此不需要decode\n",
        "\n",
        "# dataset = dataset.map(lambda words, size: (table.lookup(words), size))\n",
        "# _iter= dataset.make_initializable_iterator()\n",
        "# init = _iter.initializer\n",
        "# wordid, length = _iter.get_next()\n",
        "# with tf.Session() as sess:\n",
        "#   sess.run(init)\n",
        "#   tf.tables_initializer().run()\n",
        "#   print(sess.run([wordid, length]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ErCrzH4O6Ce1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 抄就行了啊 别自己造轮子"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YwNOBPYci45n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0fe33b66-9aaa-43b1-a69c-dd3d0eac33d0"
      },
      "cell_type": "code",
      "source": [
        "# build vocabulary\n",
        "# python\n",
        "import string\n",
        "def vocab(files, language='en'):\n",
        "  cab = set()\n",
        "  for file in files:\n",
        "    with open(file, 'r') as f:\n",
        "      text = f.read()\n",
        "      if language =='en':\n",
        "        words = set([word.strip(string.punctuation) for word in text.split()])\n",
        "      elif language =='ch':\n",
        "        words = set([char for char in text if char is not '\\n'])\n",
        "      else:\n",
        "        raise ValueError('Not supported')\n",
        "      cab = cab.union(words)\n",
        "  return {word: idx for idx, word in enumerate(cab)}\n",
        "\n",
        "def inver_vocab(cab):\n",
        "  return {idx: word for word, idx in cab.items()}\n",
        "\n",
        "source_vocab = vocab(['source'], language='ch')\n",
        "print(source_vocab)\n",
        "target_vocab = vocab(['target'], language='en')\n",
        "print(target_vocab)\n",
        "print(inver_vocab(target_vocab))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'那': 0, '是': 1, '条': 2, '只': 3, '狗': 4, '这': 5, '猫': 6}\n",
            "{'cat': 0, 'is': 1, \"That's\": 2, 'This': 3, 'dog': 4, 'a': 5}\n",
            "{0: 'cat', 1: 'is', 2: \"That's\", 3: 'This', 4: 'dog', 5: 'a'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_V5x1FfukVE8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('source', encoding='utf-8') as f:\n",
        "  ss = []\n",
        "  for line in f:\n",
        "    ss.append([(char, source_vocab[char]) for char in line if char is not '\\n'])\n",
        "\n",
        "with open('target', encoding='utf-8') as f:\n",
        "  ts = []\n",
        "  for line in f:\n",
        "    ts.append([(word, target_vocab[word]) for word in line.split() if word is not '\\n'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4o_umk1zDvzG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "src = ss[0]\n",
        "schars, sidx = zip(*src)\n",
        "\n",
        "tsrc = ts[0]\n",
        "twords, tidx = zip(*tsrc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ui3NHsZhFDeh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "src_vac_size = len(source_vocab)\n",
        "tgt_vac_size = len(target_vocab)\n",
        "ebd_size = 10\n",
        "num_units = 11\n",
        "batch_size = 1\n",
        "max_gradient_norm = 1\n",
        "learning_rate = .0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8cwhD9dzFSEM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  \n",
        "  #encode\n",
        "  with tf.variable_scope('encoder', reuse=tf.AUTO_REUSE) as scope:\n",
        "    _src = tf.expand_dims(tf.constant(sidx),0)\n",
        "    _tgt = tf.expand_dims(tf.constant(tidx),0)\n",
        "    tlength = tf.expand_dims(tf.size(_tgt), 0)\n",
        "    seq_length = tf.size(_src)\n",
        "    ebd = tf.get_variable('src_embdding', \n",
        "                          [src_vac_size, ebd_size], \n",
        "                          dtype=tf.float32)\n",
        "    x = tf.nn.embedding_lookup(ebd, _src)\n",
        "    #bidirectional lstm\n",
        "    forword_cell = tf.nn.rnn_cell.LSTMCell(num_units, name='Fbasic_lstm_cell')\n",
        "    backword_cell = tf.nn.rnn_cell.LSTMCell(num_units, name='Bbasic_lstm_cell')\n",
        "    bi_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
        "    forword_cell, backword_cell, x, dtype=tf.float32)\n",
        "    encoder_outputs = tf.concat(bi_outputs, -1)\n",
        "    \n",
        "  #decode\n",
        "  with tf.variable_scope('decoder', reuse=tf.AUTO_REUSE) as scope:\n",
        "    decoder_cell = tf.nn.rnn_cell.LSTMCell(2*num_units)\n",
        "    t_ebd = tf.get_variable('tgt_embdding', \n",
        "                          [tgt_vac_size, ebd_size], \n",
        "                          dtype=tf.float32)\n",
        "    y = tf.nn.embedding_lookup(t_ebd, _tgt)\n",
        "    helper = tf.contrib.seq2seq.TrainingHelper(y, tlength)\n",
        "    projection_layer = tf.layers.Dense(tgt_vac_size)\n",
        "    _encoder_state = tf.nn.rnn_cell.LSTMStateTuple(c=tf.concat([encoder_state[0].c,encoder_state[1].c], -1),\n",
        "                              h=tf.concat([encoder_state[0].h,encoder_state[1].h], -1))\n",
        "    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, \n",
        "                                              helper, \n",
        "                                              _encoder_state,\n",
        "                                              output_layer=projection_layer)\n",
        "    outputs, _, _= tf.contrib.seq2seq.dynamic_decode(decoder)\n",
        "    logits = outputs.rnn_output\n",
        "    \n",
        "  #loss: cross_entropy\n",
        "  cross_ent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=_tgt, logits=logits)\n",
        "  train_loss = tf.reduce_sum(cross_ent)/batch_size\n",
        "  \n",
        "  #gradient and optimization\n",
        "  params = tf.trainable_variables()\n",
        "  gradients = tf.gradients(train_loss, params)\n",
        "  clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_gradient_norm)\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "  update_step = optimizer.apply_gradients(zip(clipped_gradients, params))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L1z4XTWfIxs1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "4fa2610b-213c-4209-ba88-f513643412dd"
      },
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "with tf.Session(graph=g) as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for i in itertools.count():\n",
        "    _, _lossi = sess.run([update_step, train_loss])\n",
        "    if i % 100 == 0:\n",
        "      print(_lossi)\n",
        "    if _lossi < .1:\n",
        "      _outf = sess.run([logits])\n",
        "      break"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.258453\n",
            "6.875393\n",
            "6.421062\n",
            "5.8247933\n",
            "5.0776534\n",
            "4.270583\n",
            "3.4546585\n",
            "2.6514919\n",
            "1.9349053\n",
            "1.3539553\n",
            "0.93535984\n",
            "0.67111194\n",
            "0.51112634\n",
            "0.4045411\n",
            "0.32889313\n",
            "0.272713\n",
            "0.229622\n",
            "0.19585845\n",
            "0.16902685\n",
            "0.14744169\n",
            "0.12984718\n",
            "0.11530785\n",
            "0.103132896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nGOvZpHqXSfW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def translate(idx, vocab):\n",
        "  return ' '.join(inver_vocab(vocab)[i] for i in idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-VyG_dPAWjEq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "93517b9a-d5cd-4efb-965b-72c0ad3885b4"
      },
      "cell_type": "code",
      "source": [
        "print(translate(sidx, source_vocab))\n",
        "#翻译\n",
        "translate(np.argmax(_outf[0][0,:,:], axis=1), target_vocab)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "这 是 只 猫\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is a cat'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "RLa7uLvDZhQJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# minimal NMT 完成\n",
        "todo:\n",
        "- multilayer LSTM\n",
        "- attention mechanism\n",
        "- residual block"
      ]
    }
  ]
}