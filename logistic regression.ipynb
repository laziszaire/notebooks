{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression是用回归变量x对logits进行线性回归\n",
    "\n",
    "$$logit(p) = wx $$\n",
    "$$logit(p) = log(\\frac{p}{1-p}) = log(odds)$$\n",
    "$$ p = logit^{-1}(wx)\n",
    "    = sigmoid(wx) \n",
    "    = \\frac{e^{wx}}{1+e^{wx}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logit函数把在[0,1]范围内的变量转换到[$-\\infty,+\\infty$]  \n",
    "sigmoid函数把[$-\\infty,+\\infty$]转换为概率  \n",
    "sigmoid相加不等于1， 因为这是两个样本属于某类的概率相加，可能两个样本都很大该概率属于1，则相加接近于2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数估计\n",
    "- 最大似然估计\n",
    "- 梯度上升"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from Chemometrics.basic import sigmoid\n",
    "from warnings import warn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, lr=.1, seed=None, epsilon=1e-4):\n",
    "        self.lr = lr\n",
    "        self.seed = seed\n",
    "        self.epsilon = epsilon\n",
    "        self.fitted_ = False\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        X, y = check_X_y(X, y)\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "        np.random.seed(self.seed)\n",
    "        self.classes_, y = np.unique(y, return_inverse=True)\n",
    "        if not self.fitted_:\n",
    "            self.w_ = np.random.randn(X.shape[1])\n",
    "        if self.w_.shape[0] != X.shape[1]:\n",
    "            self.w_ = np.random.randn(X.shape[1])\n",
    "            \n",
    "        N_run = 0\n",
    "        while True:\n",
    "            _grad_w = X.T.dot(y - sigmoid(X.dot(self.w_)))\n",
    "            if np.linalg.norm(_grad_w) <= self.epsilon:\n",
    "                break\n",
    "            #todo: line search\n",
    "            #newton\n",
    "            self.w_ += self.lr*_grad_w\n",
    "            N_run += 1\n",
    "            if N_run > 1e5:\n",
    "                warn(f'maximum steps reached, norm of gradient is {np.linalg.norm(_grad_w)}')\n",
    "                break\n",
    "        self.fitted_ = True\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def w(self):\n",
    "        if not self.fitted_:\n",
    "            warn('Not fitted')\n",
    "        return self.w_\n",
    "\n",
    "    def predict_proba(self, X, y=None):\n",
    "        X = check_array(X)\n",
    "        _score = X.dot(self.w)\n",
    "        prob = sigmoid(_score)\n",
    "        return prob\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        X = check_array(X)\n",
    "        prob = self.predict_proba(X, y)\n",
    "        if len(self.classes_) == 1:\n",
    "            return self.classes_[np.zeros_like(prob, dtype=np.int32)]\n",
    "        yhat = self.classes_[np.argmax(np.vstack([1-prob, prob]).T, axis=1)]\n",
    "        return yhat\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        return accuracy_score(y, self.predict(X))\n",
    "def sigmoid(x):\n",
    "    _exp = np.where(x > 0, np.exp(-x), np.exp(x))\n",
    "    return np.where(x > 0, 1./(1. + _exp), _exp/(_exp + 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_estimator(LogisticRegression)\n",
    "# todo: 多分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(5,20)\n",
    "w = np.random.randn(20)\n",
    "y = np.where(sigmoid(X.dot(w))>.5, 1, 0)\n",
    "y\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(epsilon=0.0001, lr=0.1, seed=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 1, 0, 0]), array([0, 1, 1, 0, 0]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(X), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献\n",
    "[1] [Logit](https://en.wikipedia.org/wiki/Logit)  \n",
    "[2] [Logistic Regression for Machine Learning](https://machinelearningmastery.com/logistic-regression-for-machine-learning/)  \n",
    "[3] [sklearn.linear_model.LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)  \n",
    "[4] [creating-your-own-estimator-scikit-learn](http://danielhnyk.cz/creating-your-own-estimator-scikit-learn/#get_paramsandset_params)  \n",
    "[5] [Rolling your own estimator (scikit-learn docs)](http://scikit-learn.org/dev/developers/contributing.html#rolling-your-own-estimator)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
