{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题：[可变长度序列的分类](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/dynamic_rnn.py)\n",
    "- 方法： dynamic RNN\n",
    "- 实现： tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 数据 \n",
    "- Class 0: linear sequences (i.e. [0, 1, 2, 3,...])\n",
    "- Class 1: random sequences (i.e. [1, 3, 10, 7,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequences:\n",
    "    def __init__(self, n_samples=1000, max_seq_len=20, min_seq_len=3,\n",
    "                max_value=1000):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.seqlen = []\n",
    "        for _ in range(n_samples):\n",
    "            _len = random.randint(min_seq_len, max_seq_len)\n",
    "            self.seqlen.append(_len)\n",
    "            \n",
    "            if random.random() < .5:\n",
    "                _start = random.randint(0, max_value - _len)\n",
    "                s = list([float(_)/max_value] for _ in range(_start, _start + _len))\n",
    "                self.labels.append([1, 0])\n",
    "            else:\n",
    "                s = [[float(random.randint(0, max_value))/max_value] for _ in range(_len)]\n",
    "                self.labels.append([0, 1])\n",
    "    \n",
    "            # pad\n",
    "            s +=  [[0]] * (max_seq_len - _len)\n",
    "            self.data.append(s)\n",
    "        self.batch_id = 0\n",
    "        \n",
    "    def next(self, batch_size):\n",
    "        if self.batch_id >= len(self.data):\n",
    "            self.batch_id = 0\n",
    "        _start, _end = self.batch_id, self.batch_id + batch_size\n",
    "        data_batch = self.data[_start : _end]\n",
    "        labels_batch = self.labels[_start : _end]\n",
    "        seqlen_batch = (self.seqlen[_start : _end])\n",
    "        self.batch_id = _end\n",
    "        return data_batch, labels_batch, seqlen_batch\n",
    "    \n",
    "    def test(self):\n",
    "        pass\n",
    "s = Sequences(max_seq_len=20, min_seq_len=2)\n",
    "s.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = .01\n",
    "training_steps = 10000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "\n",
    "max_seq_len = 20\n",
    "hidden_size = 64\n",
    "n_classes = 2\n",
    "train_set = Sequences(n_samples=1000, max_seq_len=max_seq_len)\n",
    "test_set = Sequences(n_samples=100, max_seq_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-2f380d25ca97>:27: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    _x = tf.placeholder(tf.float32, [None, max_seq_len, 1])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    seqlen = tf.placeholder(tf.int32, [None])\n",
    "    _xavier = tf.contrib.layers.xavier_initializer()\n",
    "    _zeros = tf.zeros_initializer()\n",
    "    with tf.variable_scope('rnn', reuse=tf.AUTO_REUSE):\n",
    "        U = tf.get_variable('out', shape=[hidden_size, n_classes],\n",
    "                            dtype=tf.float32, initializer=_xavier)\n",
    "        b = tf.get_variable('out_biase', shape=[n_classes], dtype=tf.float32, initializer=_zeros)\n",
    "    #todo tf.unstack    \n",
    "    x = tf.unstack(_x, max_seq_len, 1) #(value, num, axis) ==> (batch, 1)\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_size)\n",
    "    #todo tf.nn.static_rnn\n",
    "    outputs, states = tf.nn.static_rnn(lstm_cell, x, \n",
    "                                       dtype=tf.float32, sequence_length=seqlen)\n",
    "    # tf.stack(values, axis=0) ==> (max_seq_len, batch_size, 1)\n",
    "    outputs = tf.stack(outputs)\n",
    "    \n",
    "    #tf.transpose, (max_seq_len, batch_size, 1) ==> (batch_size, max_seq_len, 1) \n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "    _batch_size = tf.shape(outputs)[0]\n",
    "    index = tf.range(0, _batch_size) * max_seq_len + (seqlen - 1)\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, hidden_size]), index)\n",
    "    preds = tf.matmul(outputs, U) + b\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=y)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(cost)\n",
    "    correct_pred = tf.equal(tf.argmax(preds,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Minibatch Loss= 0.693309, Training Accuracy= 0.51562\n",
      "Step 128, Minibatch Loss= 0.692693, Training Accuracy= 0.51562\n",
      "Step 25600, Minibatch Loss= 0.693772, Training Accuracy= 0.58594\n",
      "Step 51200, Minibatch Loss= 0.694127, Training Accuracy= 0.59375\n",
      "Step 76800, Minibatch Loss= 0.694464, Training Accuracy= 0.58594\n",
      "Step 102400, Minibatch Loss= 0.694789, Training Accuracy= 0.58594\n",
      "Step 128000, Minibatch Loss= 0.695103, Training Accuracy= 0.57031\n",
      "Step 153600, Minibatch Loss= 0.695408, Training Accuracy= 0.58594\n",
      "Step 179200, Minibatch Loss= 0.695703, Training Accuracy= 0.56250\n",
      "Step 204800, Minibatch Loss= 0.695988, Training Accuracy= 0.55469\n",
      "Step 230400, Minibatch Loss= 0.696261, Training Accuracy= 0.52344\n",
      "Step 256000, Minibatch Loss= 0.696522, Training Accuracy= 0.50781\n",
      "Step 281600, Minibatch Loss= 0.696769, Training Accuracy= 0.51562\n",
      "Step 307200, Minibatch Loss= 0.696998, Training Accuracy= 0.51562\n",
      "Step 332800, Minibatch Loss= 0.697206, Training Accuracy= 0.51562\n",
      "Step 358400, Minibatch Loss= 0.697389, Training Accuracy= 0.50000\n",
      "Step 384000, Minibatch Loss= 0.697543, Training Accuracy= 0.51562\n",
      "Step 409600, Minibatch Loss= 0.697660, Training Accuracy= 0.50781\n",
      "Step 435200, Minibatch Loss= 0.697734, Training Accuracy= 0.49219\n",
      "Step 460800, Minibatch Loss= 0.697755, Training Accuracy= 0.49219\n",
      "Step 486400, Minibatch Loss= 0.697712, Training Accuracy= 0.49219\n",
      "Step 512000, Minibatch Loss= 0.697589, Training Accuracy= 0.49219\n",
      "Step 537600, Minibatch Loss= 0.697366, Training Accuracy= 0.49219\n",
      "Step 563200, Minibatch Loss= 0.697014, Training Accuracy= 0.49219\n",
      "Step 588800, Minibatch Loss= 0.696493, Training Accuracy= 0.49219\n",
      "Step 614400, Minibatch Loss= 0.695735, Training Accuracy= 0.50000\n",
      "Step 640000, Minibatch Loss= 0.694623, Training Accuracy= 0.50000\n",
      "Step 665600, Minibatch Loss= 0.692936, Training Accuracy= 0.52344\n",
      "Step 691200, Minibatch Loss= 0.690210, Training Accuracy= 0.57812\n",
      "Step 716800, Minibatch Loss= 0.685419, Training Accuracy= 0.60156\n",
      "Step 742400, Minibatch Loss= 0.676428, Training Accuracy= 0.60938\n",
      "Step 768000, Minibatch Loss= 0.660360, Training Accuracy= 0.63281\n",
      "Step 793600, Minibatch Loss= 0.636800, Training Accuracy= 0.64844\n",
      "Step 819200, Minibatch Loss= 0.606268, Training Accuracy= 0.68750\n",
      "Step 844800, Minibatch Loss= 0.565565, Training Accuracy= 0.72656\n",
      "Step 870400, Minibatch Loss= 0.531285, Training Accuracy= 0.73438\n",
      "Step 896000, Minibatch Loss= 0.513363, Training Accuracy= 0.75781\n",
      "Step 921600, Minibatch Loss= 0.503053, Training Accuracy= 0.77344\n",
      "Step 947200, Minibatch Loss= 0.496255, Training Accuracy= 0.77344\n",
      "Step 972800, Minibatch Loss= 0.491368, Training Accuracy= 0.77344\n",
      "Step 998400, Minibatch Loss= 0.487516, Training Accuracy= 0.78125\n",
      "Step 1024000, Minibatch Loss= 0.484156, Training Accuracy= 0.78125\n",
      "Step 1049600, Minibatch Loss= 0.480957, Training Accuracy= 0.78125\n",
      "Step 1075200, Minibatch Loss= 0.477738, Training Accuracy= 0.78125\n",
      "Step 1100800, Minibatch Loss= 0.474416, Training Accuracy= 0.78125\n",
      "Step 1126400, Minibatch Loss= 0.470970, Training Accuracy= 0.78906\n",
      "Step 1152000, Minibatch Loss= 0.467410, Training Accuracy= 0.79688\n",
      "Step 1177600, Minibatch Loss= 0.463767, Training Accuracy= 0.79688\n",
      "Step 1203200, Minibatch Loss= 0.460075, Training Accuracy= 0.79688\n",
      "Step 1228800, Minibatch Loss= 0.456367, Training Accuracy= 0.79688\n",
      "Step 1254400, Minibatch Loss= 0.452664, Training Accuracy= 0.78906\n"
     ]
    }
   ],
   "source": [
    "model_saved = './dynamic rnn/'\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(init)\n",
    "    #train steps\n",
    "    for step in range(training_steps):\n",
    "        batch_x, batch_y, batch_seqlen = train_set.next(batch_size)\n",
    "        a = sess.run(train_op, feed_dict={_x: batch_x, y: batch_y,\n",
    "                                       seqlen: batch_seqlen})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "        # Calculate batch accuracy & loss\n",
    "            acc, loss_ = sess.run([accuracy, cost], feed_dict={_x: batch_x, y: batch_y,\n",
    "                                                seqlen: batch_seqlen})\n",
    "            print(\"Step \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "    saver.save(sess, model_saved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation\n",
    "restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./dynamic rnn/\n",
      "Testing Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, model_saved)\n",
    "    test_data = test_set.data\n",
    "    test_label = test_set.labels\n",
    "    test_seqlen = test_set.seqlen\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={_x: test_data, y: test_label,\n",
    "                                      seqlen: test_seqlen}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## todo\n",
    "1. dropout\n",
    "2. enclosure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
